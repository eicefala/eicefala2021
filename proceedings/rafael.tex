\chapter{Neurophysiological correlates of phonemic categorization}\label{ch:rafael}
\chapterauthor[1]{Rafael Laboissière}
\begin{affils}
\chapteraffil[1]{Laboratory of Psycholgy and Neurocogtion -- CNRS}
\end{affils}

\noindent
Speech perception involves the mapping of a continuous, variable acoustic signal of speech into discrete, linguistically meaningful units. This process is known as phonemic categorization. The great variability of the speech signal is caused, among other factors, by coarticulatory phenomena in the speech production process and by communication context effects. Over the past thirty years, great strides have been made in understanding the neurophysiological basis of speech perception from neuroimaging studies of the human brain.  However, it is not clear at what stage of the auditory processing the representations of speech sound cease to be veridical, i.e., faithfully encoding fine acoustic details, and become categorical, i.e., encoding sounds into phonemic categories. It is commonly assumed that in the cochlea, in the brainstem, and in the thalamus, the auditory system processes speech sounds without differentiating them from other types of non-linguistic sound. At some point, however, the central nervous system must treat speech sounds in a special way. In this presentation, a review of recent scientific studies will be presented, revealing which regions of the cerebral cortex are involved in recoding the acoustic waveform into a more abstract phonemic representation, resulting in categorical perception. The results linked to the bilateral asymmetry of categorical perception, as well as the relationship with cortical speech processing models (à la Hickok \& Poeppel) will also be presented. Finally, the principles and limits of behavioral (phonemic identification and discrimination), computational (machine learning), and neurophysiological (electroencephalography - EEG, magnetoencephalography - MEG, functional magnetic resonance imaging - fMRI) methodologies will also be presented.

